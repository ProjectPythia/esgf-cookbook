{"version":"1","records":[{"hierarchy":{"lvl1":"ESGF Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"ESGF Cookbook"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"ESGF Cookbook"},"type":"lvl1","url":"/#esgf-cookbook","position":2},{"hierarchy":{"lvl1":"ESGF Cookbook"},"content":"\n\n\n\n\n\n\n\nThis Project Pythia Cookbook covers how to access and analyze datasets that can be accessed from Earth System Grid Federation (ESGF) cyberinfrastructure.","type":"content","url":"/#esgf-cookbook","position":3},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":4},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl2":"Motivation"},"content":"This cookbook focuses on highlighting analysis recipes, as well as data acccess methods, all accesible within the Python programming language. This cookbook also spans beyond the scope of a single Climate Model Intercomparison Project (ex. CMIP6), expanding to other experiments/datasets such as CMIP5 and obs4MIPs.","type":"content","url":"/#motivation","position":5},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":6},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl2":"Authors"},"content":"Max Grover, \n\nNathan Collier, \n\nCarsten Ehbrecht, \n\nJacqueline Nugent, \n\nGerardo Rivera Tello","type":"content","url":"/#authors","position":7},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":8},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":9},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":10},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl2":"Structure"},"content":"","type":"content","url":"/#structure","position":11},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl3":"Searching","lvl2":"Structure"},"type":"lvl3","url":"/#searching","position":12},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl3":"Searching","lvl2":"Structure"},"content":"This content includes details on how to search for datasets hosted on ESGF cyberinfrastructure.","type":"content","url":"/#searching","position":13},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl3":"Workflows","lvl2":"Structure"},"type":"lvl3","url":"/#workflows","position":14},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl3":"Workflows","lvl2":"Structure"},"content":"Scientific workflows utilizing data accessed from ESGF.","type":"content","url":"/#workflows","position":15},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":16},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nthe NIMBUS Juptyerhub or on your local machine.","type":"content","url":"/#running-the-notebooks","position":17},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":18},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nthe NIMBUS Juptyerhub, which enables the execution of a\n\n\nJupyter Book in the cloud-like infrastructure. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.","type":"content","url":"/#running-on-binder","position":19},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":20},{"hierarchy":{"lvl1":"ESGF Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\n(Replace “cookbook-example” with the title of your cookbooks)\n\nClone the https://github.com/esgf2-us/esgf-cookbook repository: git clone https://github.com/esgf2-us/esgf-cookbook.git\n\nMove into the cookbook-example directorycd esgf-cookbook\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate esgf-cookbook-dev\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":21},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf"},"type":"lvl1","url":"/notebooks/complex-search","position":0},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf"},"content":"\n\n","type":"content","url":"/notebooks/complex-search","position":1},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf"},"type":"lvl1","url":"/notebooks/complex-search#complex-searching-with-intake-esgf","position":2},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf"},"content":"","type":"content","url":"/notebooks/complex-search#complex-searching-with-intake-esgf","position":3},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/complex-search#overview","position":4},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Overview"},"content":"In this tutorial we will present an interface under design to facilitate complex searching using \n\nintake-esgf. intake-esgf is a small intake and intake-esm inspired package under development in ESGF2. Please note that there is a name collison with an existing package in PyPI and conda. You will need to install the package from \n\nsource.","type":"content","url":"/notebooks/complex-search#overview","position":5},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/complex-search#prerequisites","position":6},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nInstall Package\n\nNecessary\n\n\n\nUnderstanding of NetCDF\n\nHelpful\n\nFamiliarity with metadata structure\n\nFamiliar with \n\nintake-esm\n\nHelpful\n\nSimilar interface\n\nTransient climate response\n\nBackground\n\n\n\nTime to learn: 30 minutes","type":"content","url":"/notebooks/complex-search#prerequisites","position":7},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/complex-search#imports","position":8},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Imports"},"content":"\n\nimport intake_esgf\nfrom intake_esgf import ESGFCatalog\n\n","type":"content","url":"/notebooks/complex-search#imports","position":9},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Initializing the Catalog"},"type":"lvl2","url":"/notebooks/complex-search#initializing-the-catalog","position":10},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Initializing the Catalog"},"content":"As with intake-esm we first instantiate the catalog. However, since we will populate the catalog with search results, the catalog starts empty. Internally, we query different ESGF index nodes for information about what datasets you wish to include in your analysis. As ESGF2 is actively working on an index redesign, our catlogs by default point to a Globus (ElasticSearch) based index at ALCF (Argonne Leadership Computing Facility).\n\ncat = ESGFCatalog()\nprint(cat)\nfor ind in cat.indices: # Which indices are included?\n    print(ind)\n\nWe also provide support for connecting to the ESGF1 Solr-based indices. You may specify a server in the dictionary or multiple servers - just make sure to include True.\n\nUncommend the line setting all_indices=True to include all available indices.\n\nintake_esgf.conf.set(indices={\"esgf-node.llnl.gov\": True,\n                              \"esgf-node.ornl.gov\": True,\n                              \"esgf.ceda.ac.uk\": True})\n\nintake_esgf.conf.set(all_indices=True)  # all federated indices\n\ncat = ESGFCatalog()\nfor ind in cat.indices:\n    print(ind)\n\n","type":"content","url":"/notebooks/complex-search#initializing-the-catalog","position":11},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Populate the catalog"},"type":"lvl2","url":"/notebooks/complex-search#populate-the-catalog","position":12},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Populate the catalog"},"content":"Many times, an analysis will require several variables across multiple experiments. For example, if one were to compute the transient climate response (TCRE), you would need tempererature (tas) and carbon emissions from land (nbp) and ocean (fgco2) for a 1% CO2 increase experiment (1pctCO2) as well as the control experiment (piControl). If TCRE is not in your particular science, that is ok for this notebook. It is a motivating example and the specifics are less important than the search concepts. First, we perform a search in a familiar syntax.\n\ncat.search(\n    experiment_id=[\"piControl\", \"1pctCO2\"],\n    variable_id=[\"tas\", \"fgco2\", \"nbp\"],\n    table_id=[\"Amon\", \"Omon\", \"Lmon\"],\n)\nprint(cat)\n\nInternally, this launches simultaneous searches that are combined locally to provide a global view of what datasets are available. While the Solr indices themselves can be searched in distributed fashion, they will not report if an index has failed to return a response. As index nodes go down from time to time, this can leave you with a false impression that you have found all the datasets of interest. By managing the searches locally, intake-esgf can report back to you that an index has failed and that your results may be incomplete.\n\nIf you would like details about what intake-esgf is doing, look in the local cache directory (${HOME}/.esgf/) for a esgf.log file. This is a full history of everything that intake-esgf has searched, downloaded, or accessed. You can also look at just this session by calling session_log(). In this case you will see how long each index took to return a response and if any failed\n\nprint(cat.session_log())\n\nAt this stage of the search you have a catalog full of possibly relevant datasets for your analysis, stored in a pandas dataframe. You are free to view and manipulate this dataframe to help hone these results down. It is available to you as the df member of the ESGFCatalog. You should be careful to only remove rows as internally we could use any column in the downloading of the data. Also note that we have removed the user-facing notion of where the data is hosted. The id column of this dataframe is a list of full dataset_ids which includes the location information. At the point when you are ready to download data, we will choose locations automatically that are fastest for you.\n\ncat.df\n\n","type":"content","url":"/notebooks/complex-search#populate-the-catalog","position":13},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Model Groups"},"type":"lvl2","url":"/notebooks/complex-search#model-groups","position":14},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Model Groups"},"content":"However, intake-esgf also provides you with some tools to help locate relevant data for your analysis. When conducting these kinds of analyses, we are seeking for unique combinations of a source_id, member_id, and grid_label that have all the variables that we need. We call these model groups. In an ESGF search, it is common to find a model that has, for example, a tas for r1i1p1f1 but not a fgco2. Sorting this out is time consuming and labor intensive. So first, we provide you a function to print out all model groups with the following function.\n\ncat.model_groups().to_frame()\n\nThe function model_groups() returns a pandas Series (converted to a dataframe here for printing) with all unique combinations of (source_id,member_id,grid_label) along with the dataset count for each. This helps illustrate why it can be so difficult to locate all the data relevant to a given analysis. At the time of this writing, there are 148 model groups but relatively few of them with all 6 (2 experiments and 3 variables) datasets that we need. Furthermore, you cannot rely on a model group using r1i1p1f1 for its primary result. The results above show that UKESM does not even use f1 at all, further complicating the process of finding results.\n\nIn addition to this notion of model groups, intake-esgf provides you a method remove_incomplete() for determing which model groups you wish to keep in the current search. Internally, we will group the search results dataframe by model groups and apply a function of your design to the grouped portion of the dataframe. For example, for the current work, I could just check that there are 6 datasets in the sub-dataframe.\n\ndef shall_i_keep_it(sub_df):\n    if len(sub_df) == 6:\n        return True\n    return False\n\n\ncat.remove_incomplete(shall_i_keep_it)\ncat.model_groups().to_frame()\n\nYou could write a much more complex check--it depends on what is relevant to your analysis. The effect is that the list of possible models with consistent results is now much more manageable. This method has the added benefit of forcing the user to be concrete about which models were included in an analysis.","type":"content","url":"/notebooks/complex-search#model-groups","position":15},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Removing Additional Variants"},"type":"lvl2","url":"/notebooks/complex-search#removing-additional-variants","position":16},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Removing Additional Variants"},"content":"It may also be that you wish to only include a single member_id in your analysis. The above search shows we have a few models with multiple variants that have all 6 required datasets. To be fair to those that only have 1, you may wish to only keep the smallest variant. We also provide this function as part of the ESGFCatalog object.\n\ncat.remove_ensembles()\ncat.model_groups().to_frame()\n\n","type":"content","url":"/notebooks/complex-search#removing-additional-variants","position":17},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/complex-search#summary","position":18},{"hierarchy":{"lvl1":"Complex Searching with intake-esgf","lvl2":"Summary"},"content":"At this point, you would be ready to use to_dataset_dict() to download and load all datasets into a dictionary for analysis. The point of this notebook however is to expose the search capabilities. It is our goal to make annoying and time-consuming tasks easier by providing you smart interfaces for common operations. Let us \n\nknow what else is painful for you in locating relevant data for your science.","type":"content","url":"/notebooks/complex-search#summary","position":19},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray"},"type":"lvl1","url":"/notebooks/complex-search2-and-analysis","position":0},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray"},"content":"\n\n","type":"content","url":"/notebooks/complex-search2-and-analysis","position":1},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray"},"type":"lvl1","url":"/notebooks/complex-search2-and-analysis#complex-searching-with-intake-and-analysing-employing-xarray","position":2},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray"},"content":"","type":"content","url":"/notebooks/complex-search2-and-analysis#complex-searching-with-intake-and-analysing-employing-xarray","position":3},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/complex-search2-and-analysis#overview","position":4},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl2":"Overview"},"content":"This tutorial we will present access multiple historical (as an example here) data available and analyze  using intake. Put them in a dictionary format employing xarray and plotting simple area average time series over a specific region.","type":"content","url":"/notebooks/complex-search2-and-analysis#overview","position":5},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/complex-search2-and-analysis#imports","position":6},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl2":"Imports"},"content":"\n\nimport warnings\nimport intake\nfrom distributed import Client\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport dask\nxr.set_options(display_style='html')\nwarnings.filterwarnings(\"ignore\")\n\ncat_url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\ncol = intake.open_esm_datastore(cat_url)\ncol\n\ncat = col.search(experiment_id=[\"historical\"],\n    variable_id = [\"tas\"],\n    member_id = [\"r1i1p1f1\"],\n    table_id = [\"Amon\",], \n    source_id = [ \"CMCC-ESM2\", \"CanESM5\", \"CESM2\", \"CESM2-FV2\", ]\n)\n\ncat.df\n\ndset_dict = cat.to_dataset_dict(zarr_kwargs={'consolidated': True})\nlist(dset_dict.keys())\n\nds = {}\n\nfor key in dset_dict.keys():\n    # Sort the dataset by time\n    sorted_dataset = dset_dict[key].sortby(\"time\")\n    \n    # Subset data for years 1900-2000\n    ds[key] = sorted_dataset.sel(time=slice(\"1900\", \"2000\"))\n        \n    # Optional: Print a message indicating dataset processing\n    print(f\"Processing dataset: {key}\")\n\n\nds now contains subset of datasets for each key in dset_dict\n\nLet’s check ds\n\nds\n\n","type":"content","url":"/notebooks/complex-search2-and-analysis#imports","position":7},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl3":"Calculate regional mean for each dataset and visualizing time series","lvl2":"Imports"},"type":"lvl3","url":"/notebooks/complex-search2-and-analysis#calculate-regional-mean-for-each-dataset-and-visualizing-time-series","position":8},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl3":"Calculate regional mean for each dataset and visualizing time series","lvl2":"Imports"},"content":"\n\nregn_mean = {} \nfor key in dset_dict.keys():\n    regn_mean[key] = ds[key]['tas'].sel(lon=slice(65, 100), lat=slice(5, 25)).mean(dim=['lon', 'lat']).squeeze()\n\n\nregn_mean\n\nplt.rcParams['figure.figsize'] = [15, 4]\nplt.rcParams['lines.linewidth'] = 2\nplt.rcParams['font.size'] = 10\nplt.rcParams['font.weight'] = 'bold'\n\n\n","type":"content","url":"/notebooks/complex-search2-and-analysis#calculate-regional-mean-for-each-dataset-and-visualizing-time-series","position":9},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl4":"Visualizing the regional mean for each dataset","lvl3":"Calculate regional mean for each dataset and visualizing time series","lvl2":"Imports"},"type":"lvl4","url":"/notebooks/complex-search2-and-analysis#visualizing-the-regional-mean-for-each-dataset","position":10},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl4":"Visualizing the regional mean for each dataset","lvl3":"Calculate regional mean for each dataset and visualizing time series","lvl2":"Imports"},"content":"\n\nfor key, regm in regn_mean.items():\n    source_id = key.split('.')[2]\n    regm.plot(label=source_id)\n    plt.title(f\"Mean Surface Air Temperature for {source_id}\")\n    plt.xlabel('Time')\n    plt.ylabel('Temperature (K)')\n    plt.legend()\n    plt.show()\n\n","type":"content","url":"/notebooks/complex-search2-and-analysis#visualizing-the-regional-mean-for-each-dataset","position":11},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl3":"Calculating annual mean for each dataset  and visualizing time series","lvl2":"Imports"},"type":"lvl3","url":"/notebooks/complex-search2-and-analysis#calculating-annual-mean-for-each-dataset-and-visualizing-time-series","position":12},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl3":"Calculating annual mean for each dataset  and visualizing time series","lvl2":"Imports"},"content":"\n\nannual_mean = {}\nfor key, regm in regn_mean.items():\n    annual_mean[key] = regm.resample(time='Y').mean()\n\n\nannual_mean\n\n","type":"content","url":"/notebooks/complex-search2-and-analysis#calculating-annual-mean-for-each-dataset-and-visualizing-time-series","position":13},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl4":"Visualizing the regional annual mean for each dataset","lvl3":"Calculating annual mean for each dataset  and visualizing time series","lvl2":"Imports"},"type":"lvl4","url":"/notebooks/complex-search2-and-analysis#visualizing-the-regional-annual-mean-for-each-dataset","position":14},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl4":"Visualizing the regional annual mean for each dataset","lvl3":"Calculating annual mean for each dataset  and visualizing time series","lvl2":"Imports"},"content":"\n\nfor key, anmn in annual_mean.items():\n    source_id = key.split('.')[2]\n    anmn.plot(label=source_id)\n    plt.title(f\"Mean Annual Surface Air Temperature for {source_id}\")\n    plt.xlabel('Time')\n    plt.ylabel('Temperature (K)')\n    plt.legend()\n    plt.show()\n\n","type":"content","url":"/notebooks/complex-search2-and-analysis#visualizing-the-regional-annual-mean-for-each-dataset","position":15},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl4":"Visualizing the regional annual mean for each dataset in a single panel","lvl3":"Calculating annual mean for each dataset  and visualizing time series","lvl2":"Imports"},"type":"lvl4","url":"/notebooks/complex-search2-and-analysis#visualizing-the-regional-annual-mean-for-each-dataset-in-a-single-panel","position":16},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl4":"Visualizing the regional annual mean for each dataset in a single panel","lvl3":"Calculating annual mean for each dataset  and visualizing time series","lvl2":"Imports"},"content":"\n\n# Create the plot\nplt.figure(figsize=(12, 6))\n\n# Plotting the annual mean for each dataset on the same plot\nfor key, annm in annual_mean.items():\n    source_id = key.split('.')[2]\n    annm.plot(label=source_id)\n\nplt.title(\"Annual Mean Surface Air Temperature (Regional)\")\nplt.xlabel('Time')\nplt.ylabel('Temperature (K)')\nplt.legend()\nplt.show()\n\n\n","type":"content","url":"/notebooks/complex-search2-and-analysis#visualizing-the-regional-annual-mean-for-each-dataset-in-a-single-panel","position":17},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl3":"References","lvl2":"Imports"},"type":"lvl3","url":"/notebooks/complex-search2-and-analysis#references","position":18},{"hierarchy":{"lvl1":"Complex Searching with intake and analysing employing xarray","lvl3":"References","lvl2":"Imports"},"content":"Global Mean Surface Temperature","type":"content","url":"/notebooks/complex-search2-and-analysis#references","position":19},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy"},"type":"lvl1","url":"/notebooks/ex-regrid-plot","position":0},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy"},"content":"\n\n\n\n","type":"content","url":"/notebooks/ex-regrid-plot","position":1},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy"},"type":"lvl1","url":"/notebooks/ex-regrid-plot#demo-regridding-and-plotting-with-rooki-and-cartopy","position":2},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy"},"content":"\n\n\n\n","type":"content","url":"/notebooks/ex-regrid-plot#demo-regridding-and-plotting-with-rooki-and-cartopy","position":3},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/ex-regrid-plot#overview","position":4},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl2":"Overview"},"content":"In this notebook, we demonstrate how to use Rooki to regrid CMIP model data and plot it in Cartopy for two examples:\n\nRegrid two CMIP models onto the same grid\n\nCoarsen the output for one model\n\n","type":"content","url":"/notebooks/ex-regrid-plot#overview","position":5},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/ex-regrid-plot#prerequisites","position":6},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to intake-esgf\n\nNecessary\n\n\n\nIntro to Cartopy\n\nNecessary\n\n\n\nUsing Rooki to access CMIP6 data\n\nHelpful\n\nFamiliarity with rooki\n\nUnderstanding of NetCDF\n\nHelpful\n\nFamiliarity with metadata structure\n\nTime to learn: 15 minutes\n\n\n\n","type":"content","url":"/notebooks/ex-regrid-plot#prerequisites","position":7},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/ex-regrid-plot#imports","position":8},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl2":"Imports"},"content":"\n\nimport os\nimport intake_esgf\n\n# Run this on the DKRZ node in Germany, using the ESGF1 index node at LLNL\nos.environ[\"ROOK_URL\"] = \"http://rook.dkrz.de/wps\"\nintake_esgf.conf.set(indices={\"anl-dev\": False,\n                               \"ornl-dev\": False,\n                               \"esgf-node.llnl.gov\": True})\n\nimport rooki.operators as ops\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n\nimport intake_esgf\nfrom intake_esgf import ESGFCatalog\nfrom rooki import rooki\nfrom matplotlib.gridspec import GridSpec\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n\n","type":"content","url":"/notebooks/ex-regrid-plot#imports","position":9},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl2":"Example 1: Regrid two CMIP6 models onto the same grid"},"type":"lvl2","url":"/notebooks/ex-regrid-plot#example-1-regrid-two-cmip6-models-onto-the-same-grid","position":10},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl2":"Example 1: Regrid two CMIP6 models onto the same grid"},"content":"\n\nIn this example, we want to compare the historical precipitation output between two CMIP models, CESM2 and CanESM5. Here will will look at the annual mean precipitation for 2010.\n\n","type":"content","url":"/notebooks/ex-regrid-plot#example-1-regrid-two-cmip6-models-onto-the-same-grid","position":11},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Access the desired datasets using intake-esgf and rooki","lvl2":"Example 1: Regrid two CMIP6 models onto the same grid"},"type":"lvl3","url":"/notebooks/ex-regrid-plot#access-the-desired-datasets-using-intake-esgf-and-rooki","position":12},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Access the desired datasets using intake-esgf and rooki","lvl2":"Example 1: Regrid two CMIP6 models onto the same grid"},"content":"\n\nThe function and workflow to read in CMPI6 data using intake-esgf and rooki in the next few cells are adapted from \n\nintake​-esgf​-with​-rooki​.ipynb. Essentially, we use intake-esgf to find the dataset IDs we want and then subset and average them using rooki.\n\ndef separate_dataset_id(id_list):\n    rooki_id = id_list[0]\n    rooki_id = rooki_id.split(\"|\")[0]\n    #rooki_id = f\"css03_data.{rooki_id}\"  # <-- just something you have to know for now :(\n    return rooki_id\n\n\ncat = ESGFCatalog()\ncat.search(\n        activity_id='CMIP',\n        experiment_id=[\"historical\",],\n        variable_id=[\"pr\"],\n        member_id='r1i1p1f1',\n        grid_label='gn',\n        table_id=\"Amon\",\n        source_id = [ \"CESM2\", \"CanESM5\"]\n    )\n\ndsets = [separate_dataset_id(dataset) for dataset in list(cat.df.id.values)]\ndsets\n\n\nSubset the data to get the precipitation variable for 2010 and then average by time:\n\ndset_list = [[]]*len(dsets)\n\nfor i, dset_id in enumerate(dsets):\n    wf = ops.AverageByTime(\n        ops.Subset(\n            ops.Input('pr', [dset_id]),\n            time='2010/2010'\n        )\n    )\n\n    resp = wf.orchestrate()\n\n    # if it worked, add the dataset to our list\n    if resp.ok:\n        dset_list[i] = resp.datasets()[0]\n        \n    # if it failed, tell us why\n    else:\n        print(resp.status)\n\n\nPrint the dataset list to get an overview of the metadata structure:\n\nprint(dset_list)\n\n","type":"content","url":"/notebooks/ex-regrid-plot#access-the-desired-datasets-using-intake-esgf-and-rooki","position":13},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Compare the precipitation data between models","lvl2":"Example 1: Regrid two CMIP6 models onto the same grid"},"type":"lvl3","url":"/notebooks/ex-regrid-plot#compare-the-precipitation-data-between-models","position":14},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Compare the precipitation data between models","lvl2":"Example 1: Regrid two CMIP6 models onto the same grid"},"content":"\n\nFirst, let’s quickly plot the 2010 annual mean precipitation for each model to see what we’re working with. Since precipitation values vary greatly in magnitude, using a log-normalized colormap makes the data easier to visualize.\n\nfor dset in dset_list:\n    dset.pr.plot(norm=mcolors.LogNorm())\n    plt.show()\n\nUncomment and run the following cell. If we try to take the difference outright, it fails!\n\n# pr_diff = dset_list[0].pr - dset_list[1].pr\n\nThe models have different grids so we can’t directly subtract the data. We can use the grid attribute to get information on which grid each uses.\n\nprint(dset_list[0].grid)\nprint(dset_list[1].grid)\n\n","type":"content","url":"/notebooks/ex-regrid-plot#compare-the-precipitation-data-between-models","position":15},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Regrid the models onto the same grid with Rooki","lvl2":"Example 1: Regrid two CMIP6 models onto the same grid"},"type":"lvl3","url":"/notebooks/ex-regrid-plot#regrid-the-models-onto-the-same-grid-with-rooki","position":16},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Regrid the models onto the same grid with Rooki","lvl2":"Example 1: Regrid two CMIP6 models onto the same grid"},"content":"\n\nLook at the documentation on the regrid operator to see the available grid types and regrid methods:\n\nrooki.regrid?\n\nHere we’ll do the same process as before to read in and subset the datasets with rooki, but now we regrid using ops.Regrid before averaging over time. In this example, we use method=nearest_s2d to regrid each model onto the target grid using a nearest neighbors method. The target grid is a 1.25° grid, specified by grid='1pt25deg'.\n\nrg_list = [[]]*len(dsets)\n\nfor i, dset_id in enumerate(dsets):\n    wf = ops.AverageByTime(\n        ops.Regrid(\n            ops.Subset(\n                ops.Input('pr', [dset_id]),\n                time='2010/2010'\n            ),\n            method='nearest_s2d',\n            grid='1pt25deg'\n        )\n    )\n\n\n    resp = wf.orchestrate()\n    \n    # if it worked, add the regridded dataset to our list\n    if resp.ok:\n        rg_list[i] = resp.datasets()[0]\n        \n    # if it failed, tell us why\n    else:\n        print(resp.status)\n        \n\nPrint the list of regridded datasets to get an overview of the metadata structure. Note how lat and lon are now the same and each dataset has additional attributes, including grid_original and regrid_operation, which all keep track of the regridding operations we just completed.\n\nprint(rg_list)\n\nNow they are on the same grid!\n\nprint(rg_list[0].grid)\nprint(rg_list[1].grid)\n\n","type":"content","url":"/notebooks/ex-regrid-plot#regrid-the-models-onto-the-same-grid-with-rooki","position":17},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Quick plot the before and after for each model","lvl2":"Example 1: Regrid two CMIP6 models onto the same grid"},"type":"lvl3","url":"/notebooks/ex-regrid-plot#quick-plot-the-before-and-after-for-each-model","position":18},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Quick plot the before and after for each model","lvl2":"Example 1: Regrid two CMIP6 models onto the same grid"},"content":"The plots largely look the same, as they should - with the nearest neighbors method, we are just shifting the precipitation data onto a different grid without averaging between grid cells.\n\nprint(dset_list[0].source_id)\nfor ds in [dset_list[0], rg_list[0]]:\n    ds.pr.plot(norm=mcolors.LogNorm())\n    plt.show()\n\n\nprint(dset_list[1].source_id)\nfor ds in [dset_list[1], rg_list[1]]:\n    ds.pr.plot(norm=mcolors.LogNorm())\n    plt.show()\n\n\n","type":"content","url":"/notebooks/ex-regrid-plot#quick-plot-the-before-and-after-for-each-model","position":19},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl4":"Take the difference between precipitation datasets and plot it","lvl3":"Quick plot the before and after for each model","lvl2":"Example 1: Regrid two CMIP6 models onto the same grid"},"type":"lvl4","url":"/notebooks/ex-regrid-plot#take-the-difference-between-precipitation-datasets-and-plot-it","position":20},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl4":"Take the difference between precipitation datasets and plot it","lvl3":"Quick plot the before and after for each model","lvl2":"Example 1: Regrid two CMIP6 models onto the same grid"},"content":"Now that both models are on the same grid, we can subtract the precipitation datasets and plot the difference!\n\npr_diff = rg_list[0] - rg_list[1]\n\npr_diff.pr.plot(cmap=\"bwr\")\nplt.show()\n\n\n","type":"content","url":"/notebooks/ex-regrid-plot#take-the-difference-between-precipitation-datasets-and-plot-it","position":21},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Plot everything together","lvl2":"Example 1: Regrid two CMIP6 models onto the same grid"},"type":"lvl3","url":"/notebooks/ex-regrid-plot#plot-everything-together","position":22},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Plot everything together","lvl2":"Example 1: Regrid two CMIP6 models onto the same grid"},"content":"Plot the regridded precipitation data as well as the difference between models on the same figure. We can use Cartopy to make it pretty. With GridSpec, we can also split up the figure and organize it to use the same colorbar for more than one panel.\n\n# set up figure\nfig = plt.figure(figsize=(6, 8))\ngs = GridSpec(3, 2, width_ratios=[1, 0.1], hspace=0.2)\n\n# specify the projection\nproj = ccrs.Mollweide()\n\n# set up plots for each model\naxpr_1 = plt.subplot(gs[0, 0], projection=proj)\naxpr_2 = plt.subplot(gs[1, 0], projection=proj)\naxdiff = plt.subplot(gs[2, 0], projection=proj)\n\n# axes where the colorbar will go \naxcb_pr = plt.subplot(gs[:2, 1]) \naxcb_diff = plt.subplot(gs[2, 1])\naxcb_pr.axis(\"off\")\naxcb_diff.axis(\"off\")\n\n# plot the precipitation for both models\nfor i, ax in enumerate([axpr_1, axpr_2]):\n    ds_rg = rg_list[i]\n    pcm = ax.pcolormesh(ds_rg.lon, ds_rg.lat, ds_rg.pr.isel(time=0), norm=mcolors.LogNorm(vmin=1e-7, vmax=3e-4),\n                         transform=ccrs.PlateCarree()\n                       )\n    ax.set_title(ds_rg.parent_source_id)\n    ax.add_feature(cfeature.COASTLINE)\n    \n# now plot the difference\npcmd = axdiff.pcolormesh(pr_diff.lon, pr_diff.lat, pr_diff.pr.isel(time=0), cmap=\"bwr\", vmin=-3e-4, vmax=3e-4,\n                         transform=ccrs.PlateCarree()\n                        )\naxdiff.set_title(\"{a} - {b}\".format(a=rg_list[0].parent_source_id, b=rg_list[1].parent_source_id))\naxdiff.add_feature(cfeature.COASTLINE)\n\n# set the precipitation colorbar\naxcb_pr_ins = inset_axes(axcb_pr, width=\"50%\", height=\"75%\", loc=\"center\")\ncbar_pr = plt.colorbar(pcm, cax=axcb_pr_ins, orientation=\"vertical\", extend=\"both\")\ncbar_pr.set_label(\"{n} ({u})\".format(n=rg_list[0].pr.long_name, u=rg_list[0].pr.units))\n\n# set the difference colorbar\naxcb_diff_ins = inset_axes(axcb_diff, width=\"50%\", height=\"100%\", loc=\"center\")\ncbar_diff = plt.colorbar(pcmd, cax=axcb_diff_ins, orientation=\"vertical\", extend=\"both\")\ncbar_diff.set_label(\"Difference ({u})\".format(u=pr_diff.pr.units))\n\nplt.show()\n\n\n","type":"content","url":"/notebooks/ex-regrid-plot#plot-everything-together","position":23},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl2":"Example 2: Coarsen the output for one model"},"type":"lvl2","url":"/notebooks/ex-regrid-plot#example-2-coarsen-the-output-for-one-model","position":24},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl2":"Example 2: Coarsen the output for one model"},"content":"\n\nWe can also use Rooki to regrid the data from one model onto a coarser grid. In this case, it may make more sense to use a conservative regridding method, which will conserve the physical fluxes between grid cells, rather than the nearest neighbors method we used in Example 1.\n\n","type":"content","url":"/notebooks/ex-regrid-plot#example-2-coarsen-the-output-for-one-model","position":25},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Get the data using intake-esgf and Rooki again","lvl2":"Example 2: Coarsen the output for one model"},"type":"lvl3","url":"/notebooks/ex-regrid-plot#get-the-data-using-intake-esgf-and-rooki-again","position":26},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Get the data using intake-esgf and Rooki again","lvl2":"Example 2: Coarsen the output for one model"},"content":"\n\nIn this example, we’ll look at the annual mean near-surface air temperature for CESM2 in 2010.\n\ncat = ESGFCatalog()\ncat.search(\n        activity_id='CMIP',\n        experiment_id=[\"historical\",],\n        variable_id=[\"tas\"],\n        member_id='r1i1p1f1',\n        grid_label='gn',\n        table_id=\"Amon\",\n        source_id = [ \"CESM2\"]\n    )\n\ndsets = [separate_dataset_id(dataset) for dataset in list(cat.df.id.values)]\ndsets\n\n\nFirst, get the dataset with the original grid:\n\nwf = ops.AverageByTime(\n    ops.Subset(\n        ops.Input('tas', [dsets[0]]),\n        time='2010/2010'\n    )\n)\n\nresp = wf.orchestrate()\n\nif resp.ok:\n    ds_og = resp.datasets()[0]\nelse:\n    print(resp.status)\n\n\nUse the .grid attribute to get information on the native grid:\n\nds_og.grid\n\nThe native grid is 0.9°x1.25°, so let’s try coarsening to a 1.25°x1.25° grid using the conservative method:\n\nwf = ops.AverageByTime(\n    ops.Regrid(\n        ops.Subset(\n            ops.Input('tas', [dsets[0]]),\n            time='2010/2010'\n        ),\n        method='conservative',\n        grid='1pt25deg'\n    )\n)\n\nresp = wf.orchestrate()\n\nif resp.ok:\n    ds_125 = resp.datasets()[0]\nelse:\n    print(resp.status)\n    \n\nds_125.grid\n\nWe can also make it even coarser by regridding to a 2.5°x2.5° grid:\n\nwf = ops.AverageByTime(\n    ops.Regrid(\n        ops.Subset(\n            ops.Input('tas', [dsets[0]]),\n            time='2010/2010'\n        ),\n        method='conservative',\n        grid='2pt5deg'\n    )\n)\n\nresp = wf.orchestrate()\n\nif resp.ok:\n    ds_25 = resp.datasets()[0]\nelse:\n    print(resp.status)\n    \n\nds_25.grid\n\n","type":"content","url":"/notebooks/ex-regrid-plot#get-the-data-using-intake-esgf-and-rooki-again","position":27},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Plot each dataset to look at the coarsened grids","lvl2":"Example 2: Coarsen the output for one model"},"type":"lvl3","url":"/notebooks/ex-regrid-plot#plot-each-dataset-to-look-at-the-coarsened-grids","position":28},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Plot each dataset to look at the coarsened grids","lvl2":"Example 2: Coarsen the output for one model"},"content":"\n\nMake a quick plot first:\n\nfor ds in [ds_og, ds_125, ds_25]:\n    ds[\"tas\"].plot()\n    plt.show()\n    \n\n","type":"content","url":"/notebooks/ex-regrid-plot#plot-each-dataset-to-look-at-the-coarsened-grids","position":29},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Plot the coarsened datsets together using Cartopy","lvl2":"Example 2: Coarsen the output for one model"},"type":"lvl3","url":"/notebooks/ex-regrid-plot#plot-the-coarsened-datsets-together-using-cartopy","position":30},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl3":"Plot the coarsened datsets together using Cartopy","lvl2":"Example 2: Coarsen the output for one model"},"content":"\n\nNow let’s zoom in on a smaller region, the continental US, to get a clear view of the difference in grid resolution. Here we can also decrease the colorbar limits to better see how the variable tas varies within the smaller region.\n\n# set up the figure\nfig = plt.figure(figsize=(6, 8))\ngs = GridSpec(3, 2, width_ratios=[1, 0.1], height_ratios=[1, 1, 1], hspace=0.3, wspace=0.2)\n\n# specify the projection\nproj = ccrs.PlateCarree()\n\n# set up plot axes\nax1 = plt.subplot(gs[0, 0], projection=proj)\nax2 = plt.subplot(gs[1, 0], projection=proj)\nax3 = plt.subplot(gs[2, 0], projection=proj)\naxes_list = [ax1, ax2, ax3]\n\n# set up colorbar axis\naxcb = plt.subplot(gs[:, 1])\n\n# loop through each dataset and its corresponding axis\nfor i, dset in enumerate([ds_og, ds_125, ds_25]):\n    plot_ds = dset.tas.isel(time=0)\n    ax = axes_list[i]\n    pcm = ax.pcolormesh(plot_ds.lon, plot_ds.lat, plot_ds, vmin=270, vmax=302.5, transform=proj)\n    \n    # add borders and coastlines\n    ax.add_feature(cfeature.BORDERS)\n    ax.coastlines()\n    \n    # limit to CONUS for this example\n    ax.set_xlim(-130, -60)\n    ax.set_ylim(22, 52)\n    \n    # add grid labels on bottom & left only\n    gl = ax.gridlines(color=\"None\", draw_labels=True)\n    gl.top_labels = False\n    gl.right_labels = False\n    \n    # label with the regrid type; if it fails, that means it hasn't been regridded\n    # (so label with the grid attribute instead)\n    try:\n        ax.set_title(dset.regrid_operation)\n    except:\n        ax.set_title(dset.grid)\n        \n# use the same colorbar for all plots\naxcb.axis(\"off\")\naxcb_ins = inset_axes(axcb, width=\"50%\", height=\"75%\", loc=\"center\")\ncbar = plt.colorbar(pcm, cax=axcb_ins, orientation=\"vertical\", extend=\"both\")\ncbar.set_label(\"{n} ({u})\".format(n=plot_ds.long_name, u=plot_ds.units))\n        \nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/ex-regrid-plot#plot-the-coarsened-datsets-together-using-cartopy","position":31},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/ex-regrid-plot#summary","position":32},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl2":"Summary"},"content":"Rooki offers a quick and easy way to regrid CMIP model data that can be located using intake-esgf. Cartopy lets us easily customize the plot to neatly display the geospatial data.\n\n","type":"content","url":"/notebooks/ex-regrid-plot#summary","position":33},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/ex-regrid-plot#resources-and-references","position":34},{"hierarchy":{"lvl1":"Demo: Regridding and Plotting with Rooki and Cartopy","lvl2":"Resources and references"},"content":"Regridding overview from NCAR, including descriptions of various regridding methods\n\nRooki regridding example notebook\n\nRooki documentation\n\nCartopy logo image source\n\nRooki logo image source","type":"content","url":"/notebooks/ex-regrid-plot#resources-and-references","position":35},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1},{"hierarchy":{"lvl1":"Introduction to intake-esgf"},"type":"lvl1","url":"/notebooks/intro-search","position":0},{"hierarchy":{"lvl1":"Introduction to intake-esgf"},"content":"\n\n","type":"content","url":"/notebooks/intro-search","position":1},{"hierarchy":{"lvl1":"Introduction to intake-esgf"},"type":"lvl1","url":"/notebooks/intro-search#introduction-to-intake-esgf","position":2},{"hierarchy":{"lvl1":"Introduction to intake-esgf"},"content":"","type":"content","url":"/notebooks/intro-search#introduction-to-intake-esgf","position":3},{"hierarchy":{"lvl1":"Introduction to intake-esgf","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/intro-search#overview","position":4},{"hierarchy":{"lvl1":"Introduction to intake-esgf","lvl2":"Overview"},"content":"In this tutorial we will discuss the basic functionality of \n\nintake-esgf and describe some of what it is doing under the hood. intake-esgf is an intake and intake-esm inspired package under development in ESGF2. Please note that there is a name collison with an existing package in PyPI and conda. You will need to install the package from \n\nsource.","type":"content","url":"/notebooks/intro-search#overview","position":5},{"hierarchy":{"lvl1":"Introduction to intake-esgf","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/intro-search#prerequisites","position":6},{"hierarchy":{"lvl1":"Introduction to intake-esgf","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nInstall Package\n\nNecessary\n\npip install git+https://github.com/esgf2-us/intake-esgf\n\nFamiliar with \n\nintake-esm\n\nHelpful\n\nSimilar interface\n\nUnderstanding of NetCDF\n\nHelpful\n\nFamiliarity with metadata structure\n\nTime to learn: 30 minutes","type":"content","url":"/notebooks/intro-search#prerequisites","position":7},{"hierarchy":{"lvl1":"Introduction to intake-esgf","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/intro-search#imports","position":8},{"hierarchy":{"lvl1":"Introduction to intake-esgf","lvl2":"Imports"},"content":"\n\nfrom intake_esgf import ESGFCatalog\nimport matplotlib.pyplot as plt\n\n","type":"content","url":"/notebooks/intro-search#imports","position":9},{"hierarchy":{"lvl1":"Introduction to intake-esgf","lvl2":"Populate the Catalog"},"type":"lvl2","url":"/notebooks/intro-search#populate-the-catalog","position":10},{"hierarchy":{"lvl1":"Introduction to intake-esgf","lvl2":"Populate the Catalog"},"content":"Unlike intake-esm, our catalogs initialize empty. This is because while intake-esm\nloads a large file-based database into memory, we are going to populate a catalog by\nsearching one or many index nodes. The ESGFCatalog is configured by default to query\na Globus (ElasticSearch) based index which has information about holdings at the (Argonne Leadership Computing Facility (ALCF) only. We will demonstrate how this may be expanded to include other nodes later.\n\ncat = ESGFCatalog()\nprint(cat)  # <-- nothing to see here yet\n\ncat.search(\n    experiment_id=\"historical\",\n    source_id=\"CanESM5\",\n    frequency=\"mon\",\n    variable_id=[\"gpp\", \"tas\", \"pr\"],\n)\nprint(cat)\n\nThe search has populated the catalog where results are stored internally as a pandas dataframe, where the columns are the facets common to ESGF. Printing the catalog will display each column as well as a possibly-truncated list of unique values. We can use these to help narrow down our search. In this case, we neglected to mention a member_id (also known as a variant_label). So we can repeat our search with this additional facet. Note that searches are not cumulative and so we need to repeat the previous facets in this subsequent search. Also, while for the tutorial’s sake we repeat the search here, in your own analysis codes, you could simply edit your previous search.\n\ncat.search(\n    experiment_id=\"historical\",\n    source_id=\"CanESM5\",\n    frequency=\"mon\",\n    variable_id=[\"gpp\", \"tas\", \"pr\"],\n    variant_label=\"r1i1p1f1\",  # addition from the last search\n)\nprint(cat)\n\n","type":"content","url":"/notebooks/intro-search#populate-the-catalog","position":11},{"hierarchy":{"lvl1":"Introduction to intake-esgf","lvl2":"Obtaining the datasets"},"type":"lvl2","url":"/notebooks/intro-search#obtaining-the-datasets","position":12},{"hierarchy":{"lvl1":"Introduction to intake-esgf","lvl2":"Obtaining the datasets"},"content":"Now we see that our search has located 3 datasets and thus we are ready to load these into memory. Like intake-esm, the catalog will generate a dictionary of xarray datasets. Internally, the catalog is again communicating with the index node and requesting file information. This includes which file or files are part of the datasets, their local paths, download locations, and verification information. We then try to make an optimal decision in getting the data to you as quickly as we can.\n\nIf you are running on a resource with direct access to the ESGF holdings (such a Jupyter notebook on \n\nnimbus.llnl.gov), then we check if the dataset files are locally available. We have a handful of locations built-in to intake-esgf but you can also set a location manually with cat.set_esgf_data_root().\n\nIf a dataset has associated files that have been previously downloaded into the local cache, then we will load these files into memory.\n\nIf no direct file access is found, then we will queue the dataset files for download. File downloads will occur in parallel from the locations which provide you the fastest transfer speeds. Initially we will randomize the download locations, but as you use intake-esgf, we keep track of which servers provide you fastest transfer speeds and future downloads will prefer these locations. Once downloaded, we check file validity, and load into xarray containers.\n\ndsd = cat.to_dataset_dict()\n\nYou will notice that progress bars inform you that file information is being obtained\nand that downloads are taking place. As files are downloaded, they are placed into a\nlocal cache in ${HOME}/.esgf in a directory structure that mirrors that of the\nremote storage. For future analysis which uses these datasets, intake-esgf will\nfirst check this cache to see if a file already exists and use it instead of\nre-downloading. Then it returns a dictionary whose keys are by default the minimal set\nof facets to uniquely describe a dataset in the current search.\n\nprint(dsd.keys())\n\nDuring the download process, you may have also noticed that a progress bar informed\nyou that we were adding cell measures. If you have worked with ESGF data before, you\nknow that cell measure information like areacella is needed to take proper\narea-weighted means/summations. Yet many times, model centers have not uploaded this\ninformation uniformly in all submissions. We perform a search for each dataset being\nplaced in the dataset dictionary, progressively dropping dataset facets to find, if\npossible, the cell measures that are closest to the dataset being downloaded.\nSometimes they are simply in another variant_label, but other times they could be in a\ndifferent activity_id. No matter where they are, we find them for you and add them\nby default (disable with to_dataset_dict(add_measures=False)).\n\nWe determine which measures need downloaded by looking in the dataset attributes. Since tas is an atmospheric variable, we will see that its cell_measures = 'area: areacella'. If you print this variable you will see that measure has been added.\n\ndsd[\"Amon.tas\"]\n\nHowever, for gpp we also need the land fractions, which is detected by the presence of area: where land in the cell_methods. You will notice that both areacella and sftlf are added to Lmon.gpp.\n\ndsd[\"Lmon.gpp\"]\n\n","type":"content","url":"/notebooks/intro-search#obtaining-the-datasets","position":13},{"hierarchy":{"lvl1":"Introduction to intake-esgf","lvl2":"Simple Plotting"},"type":"lvl2","url":"/notebooks/intro-search#simple-plotting","position":14},{"hierarchy":{"lvl1":"Introduction to intake-esgf","lvl2":"Simple Plotting"},"content":"\n\nfig, axs = plt.subplots(figsize=(6, 12), nrows=3)\n\n# temperature\nds = dsd[\"Amon.tas\"][\"tas\"].mean(dim=\"time\") - 273.15  # to [C]\nds.plot(ax=axs[0], cmap=\"bwr\", vmin=-40, vmax=40, cbar_kwargs={\"label\": \"tas [C]\"})\n\n# precipitation\nds = dsd[\"Amon.pr\"][\"pr\"].mean(dim=\"time\") * 86400 / 999.8 * 1000  # to [mm d-1]\nds.plot(ax=axs[1], cmap=\"Blues\", vmax=10, cbar_kwargs={\"label\": \"pr [mm d-1]\"})\n\n# gross primary productivty\nds = dsd[\"Lmon.gpp\"][\"gpp\"].mean(dim=\"time\") * 86400 * 1000  # to [g m-2 d-1]\nds.plot(ax=axs[2], cmap=\"Greens\", cbar_kwargs={\"label\": \"gpp [g m-2 d-1]\"})\n\nplt.tight_layout();\n\n","type":"content","url":"/notebooks/intro-search#simple-plotting","position":15},{"hierarchy":{"lvl1":"Introduction to intake-esgf","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/intro-search#summary","position":16},{"hierarchy":{"lvl1":"Introduction to intake-esgf","lvl2":"Summary"},"content":"intake-esgf becomes the way that you download or locate data as well as load it into memory. It is a full specification of what your analysis is about and makes your script portable to other machines or even in use with serverside computing. We are actively developing this codebase. Let us \n\nknow what other features you would like to see.","type":"content","url":"/notebooks/intro-search#summary","position":17},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data"},"type":"lvl1","url":"/notebooks/rooki","position":0},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data"},"content":"","type":"content","url":"/notebooks/rooki","position":1},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/rooki#overview","position":2},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl2":"Overview"},"content":"Rooki is a Python client to interact with \n\nRook data subsetting service for climate model data. This service is used in the backend by the \n\nEuropean Copernicus Climate Data Store to access the CMIP6 data pool. The Rook service is deployed for load-balancing at IPSL (Paris) and DKRZ (Hamburg). The CMIP6 data pool is shared with ESGF. The provided CMIP6 subset for Copernicus is synchronized at both sites.\n\nRook provides operators for subsetting, averaging and regridding to retrieve a subset of the CMIP6 data pool. These operators are implemented by the \n\nclisops Python libray and are based on \n\nxarray. The clisops library is developed by Ouranos (Canada), CEDA (UK) and DKRZ (Germany).\n\nThe operators can be called remotly using the \n\nOGC Web Processing Service (WPS) standard.\n\nROOK: Remote Operations On Klimadaten\n\nRook: \n\nhttps://​github​.com​/roocs​/rook\n\nRooki: \n\nhttps://​github​.com​/roocs​/rooki\n\nClisops: \n\nhttps://​github​.com​/roocs​/clisops\n\nRook Presentation: \n\nRook​_C3S2​_380​_2022​-02​-11​.pdf\n\n","type":"content","url":"/notebooks/rooki#overview","position":3},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/rooki#prerequisites","position":4},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Xarray\n\nNecessary\n\n\n\nUnderstanding of NetCDF\n\nHelpful\n\nFamiliarity with metadata structure\n\nKnowing OGC services\n\nHelpful\n\nUnderstanding of the service interfaces\n\nTime to learn: 15 minutes\n\n","type":"content","url":"/notebooks/rooki#prerequisites","position":5},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl2":"Init Rooki"},"type":"lvl2","url":"/notebooks/rooki#init-rooki","position":6},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl2":"Init Rooki"},"content":"\n\nimport os\n\n# Configuration line to set the wps node - in this case, use DKRZ in Germany\nos.environ['ROOK_URL'] = 'http://rook.dkrz.de/wps'\n\nfrom rooki import rooki\n\n","type":"content","url":"/notebooks/rooki#init-rooki","position":7},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl2":"Retrieve subset of CMIP6 data"},"type":"lvl2","url":"/notebooks/rooki#retrieve-subset-of-cmip6-data","position":8},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl2":"Retrieve subset of CMIP6 data"},"content":"The CMIP6 dataset is identified by a dataset-id. An intake catalog as available to lookup the available datasets:\n\nhttps://​nbviewer​.org​/github​/roocs​/rooki​/blob​/master​/notebooks​/demo​/demo​-intake​-catalog​.ipynb\n\nresp = rooki.subset(\n    collection='c3s-cmip6.CMIP.MPI-M.MPI-ESM1-2-HR.historical.r1i1p1f1.Amon.tas.gn.v20190710',\n    time='2000-01-01/2000-01-31',\n    area='-30,-40,70,80',\n)\nresp.ok\n\n","type":"content","url":"/notebooks/rooki#retrieve-subset-of-cmip6-data","position":9},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Open Dataset with xarray","lvl2":"Retrieve subset of CMIP6 data"},"type":"lvl3","url":"/notebooks/rooki#open-dataset-with-xarray","position":10},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Open Dataset with xarray","lvl2":"Retrieve subset of CMIP6 data"},"content":"\n\nds = resp.datasets()[0]\nds\n\n","type":"content","url":"/notebooks/rooki#open-dataset-with-xarray","position":11},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Plot CMIP6 Dataset","lvl2":"Retrieve subset of CMIP6 data"},"type":"lvl3","url":"/notebooks/rooki#plot-cmip6-dataset","position":12},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Plot CMIP6 Dataset","lvl2":"Retrieve subset of CMIP6 data"},"content":"\n\nds.tas.isel(time=0).plot()\n\n","type":"content","url":"/notebooks/rooki#plot-cmip6-dataset","position":13},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Show Provenance","lvl2":"Retrieve subset of CMIP6 data"},"type":"lvl3","url":"/notebooks/rooki#show-provenance","position":14},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Show Provenance","lvl2":"Retrieve subset of CMIP6 data"},"content":"A provenance document is generated remotely to document the operation steps.\nThe provenance uses the \n\nW3C PROV standard.\n\nfrom IPython.display import Image\nImage(resp.provenance_image())\n\n","type":"content","url":"/notebooks/rooki#show-provenance","position":15},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl2":"Run workflow with subset and average operator"},"type":"lvl2","url":"/notebooks/rooki#run-workflow-with-subset-and-average-operator","position":16},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl2":"Run workflow with subset and average operator"},"content":"Instead of running a single operator one can also chain several operators in a workflow.\n\n","type":"content","url":"/notebooks/rooki#run-workflow-with-subset-and-average-operator","position":17},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Use rooki operators to create a workflow","lvl2":"Run workflow with subset and average operator"},"type":"lvl3","url":"/notebooks/rooki#use-rooki-operators-to-create-a-workflow","position":18},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Use rooki operators to create a workflow","lvl2":"Run workflow with subset and average operator"},"content":"\n\nfrom rooki import operators as ops\n\n","type":"content","url":"/notebooks/rooki#use-rooki-operators-to-create-a-workflow","position":19},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Define the workflow","lvl2":"Run workflow with subset and average operator"},"type":"lvl3","url":"/notebooks/rooki#define-the-workflow","position":20},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Define the workflow","lvl2":"Run workflow with subset and average operator"},"content":"... internally the workflow tree is a json document\n\ntas = ops.Input(\n    'tas', ['c3s-cmip6.CMIP.MPI-M.MPI-ESM1-2-HR.historical.r1i1p1f1.Amon.tas.gn.v20190710']\n)\n\nwf = ops.Subset(\n    tas, \n    time=\"2000/2000\",\n    time_components=\"month:jan,feb,mar\",\n    area='-30,-40,70,80',  \n)\n\nwf = ops.WeightedAverage(wf)\n\n","type":"content","url":"/notebooks/rooki#define-the-workflow","position":21},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Optional: look at the workflow json document","lvl2":"Run workflow with subset and average operator"},"type":"lvl3","url":"/notebooks/rooki#optional-look-at-the-workflow-json-document","position":22},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Optional: look at the workflow json document","lvl2":"Run workflow with subset and average operator"},"content":"... only to give some insight\n\nimport json\nprint(json.dumps(wf._tree(), indent=4))\n\n","type":"content","url":"/notebooks/rooki#optional-look-at-the-workflow-json-document","position":23},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Submit workflow job","lvl2":"Run workflow with subset and average operator"},"type":"lvl3","url":"/notebooks/rooki#submit-workflow-job","position":24},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Submit workflow job","lvl2":"Run workflow with subset and average operator"},"content":"\n\nresp = wf.orchestrate()\nresp.ok\n\n","type":"content","url":"/notebooks/rooki#submit-workflow-job","position":25},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Open as xarray dataset","lvl2":"Run workflow with subset and average operator"},"type":"lvl3","url":"/notebooks/rooki#open-as-xarray-dataset","position":26},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Open as xarray dataset","lvl2":"Run workflow with subset and average operator"},"content":"\n\nds = resp.datasets()[0]\nds\n\n","type":"content","url":"/notebooks/rooki#open-as-xarray-dataset","position":27},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Plot dataset","lvl2":"Run workflow with subset and average operator"},"type":"lvl3","url":"/notebooks/rooki#plot-dataset","position":28},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Plot dataset","lvl2":"Run workflow with subset and average operator"},"content":"\n\nds.tas.plot()\n\n","type":"content","url":"/notebooks/rooki#plot-dataset","position":29},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Show provenance","lvl2":"Run workflow with subset and average operator"},"type":"lvl3","url":"/notebooks/rooki#show-provenance-1","position":30},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"Show provenance","lvl2":"Run workflow with subset and average operator"},"content":"\n\nImage(resp.provenance_image())\n\n","type":"content","url":"/notebooks/rooki#show-provenance-1","position":31},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/rooki#summary","position":32},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl2":"Summary"},"content":"In this notebook, we used the Rooki Python client to retrieve a subset of a CMIP6 dataset. The operations are executed remotely on a Rook subsetting service (using OGC API and xarray/clisops). The dataset is plotted and a provenance document is shown. We also showed that remote operators can be chained to be executed in a single workflow operation.","type":"content","url":"/notebooks/rooki#summary","position":33},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/rooki#whats-next","position":34},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl3":"What’s next?","lvl2":"Summary"},"content":"This service is used by the European Copernicus Climate Data Store.\n\nWe need to figure out how this service can be used in the new ESGF:\n\nwhere will it be deployed?\n\nhow can it be integrated in the ESGF search (STAC catalogs, ...)\n\n???","type":"content","url":"/notebooks/rooki#whats-next","position":35},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/rooki#resources-and-references","position":36},{"hierarchy":{"lvl1":"Compute Demo: Use Rooki to access CMIP6 data","lvl2":"Resources and references"},"content":"Roocs on GitHub\n\nCopernicus Climate Data Store\n\nSTAC","type":"content","url":"/notebooks/rooki#resources-and-references","position":37},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data"},"type":"lvl1","url":"/notebooks/rooki-enso-nonlinear","position":0},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data"},"content":"","type":"content","url":"/notebooks/rooki-enso-nonlinear","position":1},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data"},"type":"lvl1","url":"/notebooks/rooki-enso-nonlinear#compute-demo-enso-nonlinearity-index-with-cmip6-data","position":2},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data"},"content":"\n\n\n\n\n\n","type":"content","url":"/notebooks/rooki-enso-nonlinear#compute-demo-enso-nonlinearity-index-with-cmip6-data","position":3},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/rooki-enso-nonlinear#overview","position":4},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Overview"},"content":"In this demo we combine multiple multiple tools described in previous cookbooks to subset, regrid and process CMIP6 data. We will be computing a measure of ENSO nonlinearity by computing the EOFs of the pacific sea surface temperature anomalies. This measure is particularly useful for characterizing models by their ability to represent different ENSO extremes (Karamperidou et al., 2017).\n\nThe process we are going to follow in this demo is:\n\nFind the CMIP6 data we need using intake-esgf\n\nSubset the data and regrid it to a common grid using Rooki\n\nLoad the datasets into xarray and perform the computations\n\nPlot the results\n\n","type":"content","url":"/notebooks/rooki-enso-nonlinear#overview","position":5},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/rooki-enso-nonlinear#prerequisites","position":6},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Xarray\n\nNecessary\n\nHow to use xarray to work with NetCDF data\n\nIntro to Intake-ESGF\n\nNecessary\n\nHow to configure a search and use output\n\nIntro to Rooki\n\nHelpful\n\nHow to initialize and run rooki\n\nIntro to EOFs\n\nHelpful\n\nUnderstanding of EOFs\n\nTime to learn: 20 minutes\n\n","type":"content","url":"/notebooks/rooki-enso-nonlinear#prerequisites","position":7},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/rooki-enso-nonlinear#imports","position":8},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Imports"},"content":"\n\nimport os\nimport intake_esgf\n\n# Run this on the DKRZ node in Germany, using the ESGF1 index node at LLNL\nos.environ[\"ROOK_URL\"] = \"http://rook.dkrz.de/wps\"\nintake_esgf.conf.set(indices={\"anl-dev\": False,\n                               \"ornl-dev\": False,\n                               \"esgf-node.llnl.gov\": True})\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.polynomial.polynomial as poly\nimport xarray as xr\nimport xeofs as xe\nfrom intake_esgf import ESGFCatalog\nfrom rooki import operators as ops\nfrom rooki import rooki\n\n","type":"content","url":"/notebooks/rooki-enso-nonlinear#imports","position":9},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Retrieve subset of CMIP6 data"},"type":"lvl2","url":"/notebooks/rooki-enso-nonlinear#retrieve-subset-of-cmip6-data","position":10},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Retrieve subset of CMIP6 data"},"content":"The CMIP6 dataset is identified by a dataset-id. Using intake-esgf we can query the ESGF database for the variables and models we are interested in. For this demo we are interested in the tos (sea surface temperature) variable for the historical runs. Also, for sake of simplicity we will only query a subset of the models available.\n\ncat = ESGFCatalog()\ncat.search(\n    experiment_id=[\"historical\"],\n    variable_id=[\"tos\"],\n    table_id=[\"Omon\"],\n    project=[\"CMIP6\"],\n    grid_label=[\"gn\"],\n    source_id=[\n        \"CAMS-CSM1-0\",\n        \"FGOALS-g3\",\n        \"CMCC-CM2-SR5\",\n        \"CNRM-CM6-1\",\n        \"CNRM-ESM2-1\",\n        \"CESM2\",\n    ],\n)\ncat.remove_ensembles()\nprint(cat)\n\nOnce the catalog has been queried, we have to do some manipulation in pandas to keep only the dataset_id. This has to be done because the same data has multiple locations online, and these get appended at the end of the dataset_id. Rookie only accepts the dataset_id without the online location, so we get rid of it in the next step.\n\ndef keep_ds_id(ds):\n    return ds[0].split(\"|\")[0]\n\ncollections = cat.df.id.apply(keep_ds_id).to_list()\ncollections\n\nWe are left with a list of dataset_ids that Rookie can accept as input for the next step.\n\n","type":"content","url":"/notebooks/rooki-enso-nonlinear#retrieve-subset-of-cmip6-data","position":11},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Subset and regrid the data"},"type":"lvl2","url":"/notebooks/rooki-enso-nonlinear#subset-and-regrid-the-data","position":12},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Subset and regrid the data"},"content":"We define a function that will do the subset and regridding for us for each of the dataset_ids we have. The function will take the dataset_id as input and then use Rookie functions to select 100 years of data for the tos variable in the Pacific Ocean region. We don’t need high resolution data for this particular use, so 2.5 degree resolution is enough.\n\ndef get_pacific_ocean(dataset_id):\n    wf = ops.Regrid(\n        ops.Subset(\n            ops.Input(\"tos\", [dataset_id]),\n            time=\"1900-01-01/2000-01-31\",\n            area=\"100,-20,280,20\",\n        ),\n        method=\"nearest_s2d\",\n        grid=\"2pt5deg\",\n    )\n    resp = wf.orchestrate()\n    if resp.ok:\n        print(f\"{resp.size_in_mb=}\")\n        ds = resp.datasets()[0]\n    else:\n        ds = xr.Dataset()\n    return ds\n\nsst_data = {dset: get_pacific_ocean(dset) for dset in collections}\n\n","type":"content","url":"/notebooks/rooki-enso-nonlinear#subset-and-regrid-the-data","position":13},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"ENSO nonlinearity measure: alpha value"},"type":"lvl2","url":"/notebooks/rooki-enso-nonlinear#enso-nonlinearity-measure-alpha-value","position":14},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"ENSO nonlinearity measure: alpha value"},"content":"\n\nThis part of the demo is computation heavy. You can refer to Takahashi et al. (2011) and Karamperidou et al. (2017) for more details on the usefulness and computation of the alpha parameter.\n\nThe alpha parameter is computed by doing a quadratic fit to the first two EOFs for the DJF season of the SST anomalies in the Pacific region. We are looking to obtain two EOFs modes that represent the Eastern and central pacific SST patterns, which is why we include a correction factor to account for the fact the sometimes the EOFs come with the opposite sign.\n\nThe higher the value of alpha, the more nonlinear (or extreme) ENSO events can be represented by the model. Likewise, a model with lower alpha values will have a harder time representing extreme ENSO events, making it not suitable for climate studies of ENSO in a warming climate (Cai et al., 2018, 2021).\n\nWe are looking to obtain data that can reproduce a figure similar to the one below (taken from Karamperiou et al., 2017):\n\n\n\nEach of the “wings” of this boomerang-shaped distribution represents a different ENSO extreme, with the left (right) wing representing the extreme central (eastern) pacific El Niño events. More details on Takahashi et al. (2011).\n\ndef compute_alpha(pc1, pc2):\n    coefs = poly.polyfit(pc1, pc2, deg=2)\n    xfit = np.arange(pc1.min(), pc1.max() + 0.1, 0.1)\n    fit = poly.polyval(xfit, coefs)\n    return coefs[-1], xfit, fit\n\n\ndef correction_factor(model):\n    _eofs = model.components()\n    _subset = dict(lat=slice(-5, 5), lon=slice(140, 180))\n    corr_factor = np.zeros(2)\n    corr_factor[0] = 1 if _eofs.sel(mode=1, **_subset).mean() > 0 else -1\n    corr_factor[1] = 1 if _eofs.sel(mode=2, **_subset).mean() > 0 else -1\n    return xr.DataArray(corr_factor, coords=[(\"mode\", [1, 2])])\n\n\ndef compute_index(ds):\n    tos = ds.tos.sel(lat=slice(-20, 20), lon=slice(100, 280))\n    tos_anom = tos.groupby(\"time.month\").apply(lambda x: x - x.mean(\"time\"))\n\n    # Compute Eofs\n    model = xe.models.EOF(n_modes=2, use_coslat=True)\n    model.fit(tos_anom, dim=\"time\")\n    corr_factor = correction_factor(model)\n    # eofs = s_model.components()\n    scale_factor = model.singular_values() / np.sqrt(model.explained_variance())\n    pcs = (\n        model.scores().convert_calendar(\"standard\", align_on=\"date\")\n        * scale_factor\n        * corr_factor\n    )\n\n    pc1 = pcs.sel(mode=1)\n    pc1 = pc1.sel(time=pc1.time.dt.month.isin([12, 1, 2]))\n    pc1 = pc1.resample(time=\"QS-DEC\").mean().dropna(\"time\")\n\n    pc2 = pcs.sel(mode=2)\n    pc2 = pc2.sel(time=pc2.time.dt.month.isin([12, 1, 2]))\n    pc2 = pc2.resample(time=\"QS-DEC\").mean().dropna(\"time\")\n\n    alpha, xfit, fit = compute_alpha(pc1, pc2)\n\n    return pc1, pc2, alpha, xfit, fit\n\nNow we can compute the alpha parameter for each of the models we have selected.\n\nalpha_fits = {}\nfor key, item in sst_data.items():\n    if len(item.variables) == 0:\n        continue\n    alpha_fits[key] = compute_index(item)\n\n","type":"content","url":"/notebooks/rooki-enso-nonlinear#enso-nonlinearity-measure-alpha-value","position":15},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Plot the results"},"type":"lvl2","url":"/notebooks/rooki-enso-nonlinear#plot-the-results","position":16},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Plot the results"},"content":"Finally, we can plot the results of the alpha parameter for each of the models we have selected. This will give us an idea of how well the models represent different ENSO extremes.\n\nfig, axs = plt.subplots(nrows=3, ncols=2, figsize=(8, 12))\naxs = axs.ravel()\nfor num, (ds, (pc1, pc2, alpha, xfit, fit)) in enumerate(alpha_fits.items()):\n    ax = axs[num]\n    ax.axhline(0, color=\"k\", linestyle=\"--\", alpha=0.2)\n    ax.axvline(0, color=\"k\", linestyle=\"--\", alpha=0.2)\n\n    # draw a line 45 degrees\n    x = np.linspace(-6, 6, 100)\n    y = x\n    ax.plot(x, y, color=\"k\", alpha=0.5, lw=1)\n    ax.plot(-x, y, color=\"k\", alpha=0.5, lw=1)\n\n    ax.scatter(\n        pc1,\n        pc2,\n        s=8,\n        marker=\"o\",\n        c=\"w\",\n        edgecolors=\"k\",\n        linewidths=0.5,\n    )\n\n    ax.plot(xfit, fit, c=\"r\", label=f\"$\\\\alpha=${alpha:.2f}\")\n\n    ax.set_xlabel(\"PC1\")\n    ax.set_ylabel(\"PC2\")\n\n    ax.set_title(ds.split(\".\")[3])\n\n    ax.set_xlim(-4, 4)\n    ax.set_ylim(-4, 4)\n    ax.legend()\nfig.subplots_adjust(hspace=0.3)\n\nFrom this example, we can see that from the subset of models we have selected, the alpha parameter is higher for CMCC-CM2-SR5 compared to the other models as the “boomerang” shape is better represented in this model. This indicates that this model is better at representing extreme ENSO events compared to the other models.\n\n","type":"content","url":"/notebooks/rooki-enso-nonlinear#plot-the-results","position":17},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/rooki-enso-nonlinear#summary","position":18},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Summary"},"content":"In this notebook, we used intake-esgf with Rooki Python client to retrieve a subset of a CMIP6 dataset. The subset and regrid operations are executed remotely on a Rook subsetting service (using OGC API and xarray/clisops). The dataset is analyzed using xeofs to extract a measurement used in ENSO research. We also showed that remote operators can be chained to be executed in a single workflow operation.","type":"content","url":"/notebooks/rooki-enso-nonlinear#summary","position":19},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Resources"},"type":"lvl2","url":"/notebooks/rooki-enso-nonlinear#resources","position":20},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"Resources"},"content":"Roocs on GitHub\n\nCopernicus Climate Data Store\n\nSTAC","type":"content","url":"/notebooks/rooki-enso-nonlinear#resources","position":21},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"References"},"type":"lvl2","url":"/notebooks/rooki-enso-nonlinear#references","position":22},{"hierarchy":{"lvl1":"Compute Demo: ENSO nonlinearity index with CMIP6 data","lvl2":"References"},"content":"Cai, W., Santoso, A., Collins, M., Dewitte, B., Karamperidou, C., Kug, J.-S., Lengaigne, M., McPhaden, M. J., Stuecker, M. F., Taschetto, A. S., Timmermann, A., Wu, L., Yeh, S.-W., Wang, G., Ng, B., Jia, F., Yang, Y., Ying, J., Zheng, X.-T., … Zhong, W. (2021). Changing El Niño–Southern Oscillation in a warming climate. Nature Reviews Earth & Environment, 2(9), 628–644. \n\nCai et al. (2021)\n\nCai, W., Wang, G., Dewitte, B., Wu, L., Santoso, A., Takahashi, K., Yang, Y., Carréric, A., & McPhaden, M. J. (2018). Increased variability of eastern Pacific El Niño under greenhouse warming. Nature, 564(7735), 201–206. \n\nCai et al. (2018)\n\nKaramperidou, C., Jin, F.-F., & Conroy, J. L. (2017). The importance of ENSO nonlinearities in tropical pacific response to external forcing. Climate Dynamics, 49(7), 2695–2704. \n\nKaramperidou et al. (2016)\n\nTakahashi, K., Montecinos, A., Goubanova, K., & Dewitte, B. (2011). ENSO regimes: Reinterpreting the canonical and Modoki El Niño. Geophysical Research Letters, 38(10). \n\nTakahashi et al. (2011)","type":"content","url":"/notebooks/rooki-enso-nonlinear#references","position":23},{"hierarchy":{"lvl1":"Running Notebooks on Nimbus"},"type":"lvl1","url":"/notebooks/running-on-nimbus","position":0},{"hierarchy":{"lvl1":"Running Notebooks on Nimbus"},"content":"Interested in running your notebooks on ESGF infrastructure? Please follow these steps!","type":"content","url":"/notebooks/running-on-nimbus","position":1},{"hierarchy":{"lvl1":"Running Notebooks on Nimbus","lvl2":"1. Apply for access to the Nimbus Access"},"type":"lvl2","url":"/notebooks/running-on-nimbus#id-1-apply-for-access-to-the-nimbus-access","position":2},{"hierarchy":{"lvl1":"Running Notebooks on Nimbus","lvl2":"1. Apply for access to the Nimbus Access"},"content":"Please fill out \n\nthis form to request access to the Nimbus Jupyterhub!\n\nYou will be added to the \n\nNimbus User Group, which is used for authentication!","type":"content","url":"/notebooks/running-on-nimbus#id-1-apply-for-access-to-the-nimbus-access","position":3},{"hierarchy":{"lvl1":"Running Notebooks on Nimbus","lvl2":"2. Clone this repository"},"type":"lvl2","url":"/notebooks/running-on-nimbus#id-2-clone-this-repository","position":4},{"hierarchy":{"lvl1":"Running Notebooks on Nimbus","lvl2":"2. Clone this repository"},"content":"Once you log into the Jupyterhub (\n\nhttps://​nimbus​.llnl​.gov/), go to your home directory (shown by default) and clone this repositorygit clone https://github.com/ProjectPythia/esgf-cookbook.git","type":"content","url":"/notebooks/running-on-nimbus#id-2-clone-this-repository","position":5},{"hierarchy":{"lvl1":"Running Notebooks on Nimbus","lvl2":"3. Build your Execution Environment"},"type":"lvl2","url":"/notebooks/running-on-nimbus#id-3-build-your-execution-environment","position":6},{"hierarchy":{"lvl1":"Running Notebooks on Nimbus","lvl2":"3. Build your Execution Environment"},"content":"The cookbook environment is slightly different than the base environment available on the hub. You will need to build the required environment, using the environment.yml file in the repository.conda env create -f esgf-cookbook/environment.yml","type":"content","url":"/notebooks/running-on-nimbus#id-3-build-your-execution-environment","position":7},{"hierarchy":{"lvl1":"Running Notebooks on Nimbus","lvl2":"Activate Your Environment"},"type":"lvl2","url":"/notebooks/running-on-nimbus#activate-your-environment","position":8},{"hierarchy":{"lvl1":"Running Notebooks on Nimbus","lvl2":"Activate Your Environment"},"content":"Once you build the enivronment, you will need to activate it. You will need to follow the following steps:# Make sure you can activate the environment\nsource .bashrc\n\n# Activate the environment\nconda activate esgf-cookbook-dev","type":"content","url":"/notebooks/running-on-nimbus#activate-your-environment","position":9},{"hierarchy":{"lvl1":"Running Notebooks on Nimbus","lvl2":"Open a Notebook and Select the esgf-cookbook-dev environment"},"type":"lvl2","url":"/notebooks/running-on-nimbus#open-a-notebook-and-select-the-esgf-cookbook-dev-environment","position":10},{"hierarchy":{"lvl1":"Running Notebooks on Nimbus","lvl2":"Open a Notebook and Select the esgf-cookbook-dev environment"},"content":"Now that you have an environment, you can select this when opening a notebook. Select in the top right corner the kernel options, and select esgf-cookbook-dev.\n\nWait a second for the notebook to pick up on this, then execute your cells!\n\nIf you need to install more packages, or update versions, you can do so by updating your environment.yml file or by installing via the command line.","type":"content","url":"/notebooks/running-on-nimbus#open-a-notebook-and-select-the-esgf-cookbook-dev-environment","position":11},{"hierarchy":{"lvl1":"Using intake-esgf with rooki"},"type":"lvl1","url":"/notebooks/use-intake-esgf-with-rooki","position":0},{"hierarchy":{"lvl1":"Using intake-esgf with rooki"},"content":"\n\n\n\n","type":"content","url":"/notebooks/use-intake-esgf-with-rooki","position":1},{"hierarchy":{"lvl1":"Using intake-esgf with rooki"},"type":"lvl1","url":"/notebooks/use-intake-esgf-with-rooki#using-intake-esgf-with-rooki","position":2},{"hierarchy":{"lvl1":"Using intake-esgf with rooki"},"content":"","type":"content","url":"/notebooks/use-intake-esgf-with-rooki#using-intake-esgf-with-rooki","position":3},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/use-intake-esgf-with-rooki#overview","position":4},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Overview"},"content":"In this notebook we will demonstrate how to use intake-esgf and rooki to perform server-side operations and return the result to the user. This will occur in several steps.\n\nWe use intake-esgf to find data which is local to the ORNL server and then form an id which rooki uses to load the data remotely.\n\nWe build a rooki workflow which uses these ids (rooki_id) to subset and average the data remotely.\n\nThe results are downloaded locally and we visualize them interactively using hvplot.\n\n","type":"content","url":"/notebooks/use-intake-esgf-with-rooki#overview","position":5},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/use-intake-esgf-with-rooki#prerequisites","position":6},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Intake-ESGF\n\nNecessary\n\nHow to configure a search and use output\n\nIntro to Rooki\n\nHelpful\n\nHow to initialize and run rooki\n\nIntro to hvPlot\n\nNecessary\n\nHow to plot interactive visualizations\n\nTime to learn: 30 minutes\n\n\n\n","type":"content","url":"/notebooks/use-intake-esgf-with-rooki#prerequisites","position":7},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/use-intake-esgf-with-rooki#imports","position":8},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Imports"},"content":"\n\nBefore importing rooki, we need to set an environment variable that will signal the rooki client to use the web processing service (WPS) deployment located at Oak Ridge National Lab (ORNL).\n\nimport os\n\n# Configuration line to set the wps node - in this case, use ORNL in the USA\nurl = \"https://esgf-node.ornl.gov/wps\"\nos.environ[\"ROOK_URL\"] = url\n\nfrom rooki import operators as ops\nfrom rooki import rooki\n\n# Other imports\nimport holoviews as hv\nimport hvplot.xarray\nimport intake_esgf\nimport matplotlib.pyplot as plt\nimport panel as pn\nimport xarray as xr\nfrom intake_esgf import ESGFCatalog\n\nhv.extension(\"bokeh\")\n\n","type":"content","url":"/notebooks/use-intake-esgf-with-rooki#imports","position":9},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Search and Find Data for Surface Temperature on the ORNL Node"},"type":"lvl2","url":"/notebooks/use-intake-esgf-with-rooki#search-and-find-data-for-surface-temperature-on-the-ornl-node","position":10},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Search and Find Data for Surface Temperature on the ORNL Node"},"content":"Let’s start with refining which index we would like to search from. For this analysis, we are remotely computing on the ORNL node since this is where rooki is running. We know this from checking the ._url method of rooki!\n\nrooki._url\n\n","type":"content","url":"/notebooks/use-intake-esgf-with-rooki#search-and-find-data-for-surface-temperature-on-the-ornl-node","position":11},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl3":"Set the Index Node and Search","lvl2":"Search and Find Data for Surface Temperature on the ORNL Node"},"type":"lvl3","url":"/notebooks/use-intake-esgf-with-rooki#set-the-index-node-and-search","position":12},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl3":"Set the Index Node and Search","lvl2":"Search and Find Data for Surface Temperature on the ORNL Node"},"content":"Because we are using the ORNL-based WPS, we only need information about ORNL holdings. So here we configure intake-esgf to only look at the ORNL index for data information.\n\nintake_esgf.conf.set(indices={\"anl-dev\": False,\n                              \"ornl-dev\": True})\n\nNow we instantiate the catalog and perform a search for surface air temperature (tas) data from a few institution’s models. Note that we have also included specificity of the data node. The ORNL index contains information about holdings beyond the ORNL data node and so we give this to force the catalog to only return information about holdings which are local to ORNL.\n\ncat = ESGFCatalog().search(\n    experiment_id=\"historical\",\n    variable_id=\"tas\",\n    member_id=\"r1i1p1f1\",\n    table_id=\"Amon\",\n    institution_id=[\"MIROC\", \"NCAR\", \"NASA-GISS\", \"CMCC\"],\n)\ncat.df\n\n","type":"content","url":"/notebooks/use-intake-esgf-with-rooki#set-the-index-node-and-search","position":13},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Extract IDs to Pass to Rooki"},"type":"lvl2","url":"/notebooks/use-intake-esgf-with-rooki#extract-ids-to-pass-to-rooki","position":14},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Extract IDs to Pass to Rooki"},"content":"The catalog returns a lot of information about the datasets that were found, but the rooki WPS interface just needs an ID that looks similar to what we find in the id column of the dataframe. We need to remove the |esgf-node.ornl.gov on the end and prepend a ccs03_data. To do this we will write a function and apply it to the dataframe.\n\ndef build_rooki_id(id_list):\n    rooki_id = id_list[0]\n    rooki_id = rooki_id.split(\"|\")[0]\n    rooki_id = f\"css03_data.{rooki_id}\"  # <-- just something you have to know for now :(\n    return rooki_id\n\nrooki_ids = cat.df.id.apply(build_rooki_id).to_list()\nrooki_ids\n\n","type":"content","url":"/notebooks/use-intake-esgf-with-rooki#extract-ids-to-pass-to-rooki","position":15},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl3":"Compute with Rooki","lvl2":"Extract IDs to Pass to Rooki"},"type":"lvl3","url":"/notebooks/use-intake-esgf-with-rooki#compute-with-rooki","position":16},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl3":"Compute with Rooki","lvl2":"Extract IDs to Pass to Rooki"},"content":"Now that we have a list of IDs to pass to rooki, let’s compute! In our case we are interested in the annual temperature from 1990-2000 over an area that includes India (latitude from 0 to 35, longitude from 65 to 100). The following function will construct a rooki workflow that uses operators (functions in the ops namespace) that rooki uses to:\n\nread in data (ops.Input)\n\nsubset in time and space (ops.Subset), and\n\naverage in time (ops.AverageByTime) on a yearly frequency.\n\nWe then check to make sure the response is okay, and if it is, return the processed dataset to the user! If something went wrong, the function will raise an error and show you the message that rooki sent back.\n\ndef india_annual_temperature(rooki_id):\n    workflow = ops.AverageByTime(\n        ops.Subset(\n            ops.Input(\"tas\", [rooki_id]),\n            time=\"1990-01-01/2000-01-01\",\n            area=\"65,0,100,35\",\n        ),\n        freq=\"year\",\n    )\n    response = workflow.orchestrate()\n    if not response.ok:\n        raise ValueError(response)\n    return response.datasets()[0]\n\nNow let’s test a single rooki_id to demonstrate successful functionality. The rooki_id let’s the WPS know on which dataset we are intersted in operating and then the data is loaded remotely, subset, and then averaged. After this computation is finished on the server, the result is transferred to you and loaded into a xarray dataset. Inspect the dataset header to see that there are 10 times, one for each year and the latitude and longitude range spans our input values.\n\nindia_annual_temperature(rooki_ids[0])\n\nNow that we have some confidence in our workflow function, we can iterate over rooki_id’s running for each and saving into a dictionary whose keys are the different models. You should see messages print to the screen which inform you where the temporary output is being downloaded. This location can be configured in rooki, but for now we will just load them into datasets.\n\ndsd = {\n    rooki_id.split(\".\")[4]: india_annual_temperature(rooki_id)\n    for rooki_id in rooki_ids\n}\n\n","type":"content","url":"/notebooks/use-intake-esgf-with-rooki#compute-with-rooki","position":17},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Visualize the Output"},"type":"lvl2","url":"/notebooks/use-intake-esgf-with-rooki#visualize-the-output","position":18},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Visualize the Output"},"content":"Let’s use hvPlot to visualize. The datasets are stored in a dictionary of datasets, we need to:\n\nExtract a single key\n\nPlot a contour filled visualization, with some geographic features\n\ntas = dsd[\"MIROC6\"].tas\ntas.hvplot.contourf(\n    x=\"lon\",\n    y=\"lat\",\n    cmap=\"Reds\",\n    levels=20,\n    clim=(250, 320),\n    features=[\"land\", \"ocean\"],\n    alpha=0.7,\n    widget_location=\"bottom\",\n    clabel=\"Yearly Average Temperature (K)\",\n    geo=True,\n)\n\n\n\n","type":"content","url":"/notebooks/use-intake-esgf-with-rooki#visualize-the-output","position":19},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/use-intake-esgf-with-rooki#summary","position":20},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Summary"},"content":"Within this notebook, we learned how to specify a specific index node to search from, pass discovered datasets to rooki, and chain remote-compute with several operations using rooki. We then visualized the output using hvPlot, leading to an interactive plot!","type":"content","url":"/notebooks/use-intake-esgf-with-rooki#summary","position":21},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/use-intake-esgf-with-rooki#whats-next","position":22},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl3":"What’s next?","lvl2":"Summary"},"content":"More adaptations of the intake-esgf + rooki to remotely compute on ESGF data.\n\n","type":"content","url":"/notebooks/use-intake-esgf-with-rooki#whats-next","position":23},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/use-intake-esgf-with-rooki#resources-and-references","position":24},{"hierarchy":{"lvl1":"Using intake-esgf with rooki","lvl2":"Resources and references"},"content":"intake-esgf documentation\n\nrooki documentation\n\nWorking with geographic data with hvPlot","type":"content","url":"/notebooks/use-intake-esgf-with-rooki#resources-and-references","position":25}]}