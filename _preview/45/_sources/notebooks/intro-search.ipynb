{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/logos/esgf2-us.png\" width=250 alt=\"ESGF logo\"></img>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to `intake-esgf`\n",
    "\n",
    "## Overview\n",
    "In this tutorial we will discuss the basic functionality of [intake-esgf](https://github.com/esgf2-us/intake-esgf) and describe some of what it is doing under the hood. `intake-esgf` is an `intake` and `intake-esm` *inspired* package under development in ESGF2. Please note that there is a name collison with an existing package in PyPI and conda. You will need to install the package from [source](https://github.com/esgf2-us/intake-esgf). \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Install Package](https://github.com/esgf2-us/intake-esgf) | Necessary | `pip install git+https://github.com/esgf2-us/intake-esgf`|\n",
    "| Familiar with [intake-esm](https://intake-esm.readthedocs.io/en/stable/) | Helpful | Similar interface |\n",
    "| [Understanding of NetCDF](https://foundations.projectpythia.org/core/data-formats/netcdf-cf.html) | Helpful | Familiarity with metadata structure |\n",
    "- **Time to learn**: 30 minutes\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intake_esgf import ESGFCatalog\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the Catalog\n",
    "\n",
    "Unlike `intake-esm`, our catalogs initialize empty. This is because while `intake-esm`\n",
    "loads a large file-based database into memory, we are going to populate a catalog by\n",
    "searching one or many index nodes. The `ESGFCatalog` is configured by default to query\n",
    "a Globus (ElasticSearch) based index which has information about holdings at the (Argonne Leadership Computing Facility (ALCF) only. We will demonstrate how this may be expanded to include other nodes later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ESGFCatalog()\n",
    "print(cat)  # <-- nothing to see here yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary information for 195 results:\n",
      "mip_era                                                     [CMIP6]\n",
      "activity_id                                                  [CMIP]\n",
      "institution_id                                              [CCCma]\n",
      "source_id                                                 [CanESM5]\n",
      "experiment_id                                          [historical]\n",
      "member_id         [r28i1p2f1, r6i1p2f1, r14i1p1f1, r20i1p2f1, r2...\n",
      "table_id                                               [Lmon, Amon]\n",
      "variable_id                                          [gpp, tas, pr]\n",
      "grid_label                                                     [gn]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "cat.search(\n",
    "    experiment_id=\"historical\",\n",
    "    source_id=\"CanESM5\",\n",
    "    frequency=\"mon\",\n",
    "    variable_id=[\"gpp\", \"tas\", \"pr\"],\n",
    ")\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search has populated the catalog where results are stored internally as a `pandas` dataframe, where the columns are the facets common to ESGF. Printing the catalog will display each column as well as a possibly-truncated list of unique values. We can use these to help narrow down our search. In this case, we neglected to mention a `member_id` (also known as a `variant_label`). So we can repeat our search with this additional facet. Note that searches are not cumulative and so we need to repeat the previous facets in this subsequent search. Also, while for the tutorial's sake we repeat the search here, in your own analysis codes, you could simply edit your previous search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.search(\n",
    "    experiment_id=\"historical\",\n",
    "    source_id=\"CanESM5\",\n",
    "    frequency=\"mon\",\n",
    "    variable_id=[\"gpp\", \"tas\", \"pr\"],\n",
    "    variant_label=\"r1i1p1f1\",  # addition from the last search\n",
    ")\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the datasets\n",
    "\n",
    "Now we see that our search has located 3 datasets and thus we are ready to load these into memory. Like `intake-esm`, the catalog will generate a dictionary of `xarray` datasets. Internally, the catalog is again communicating with the index node and requesting file information. This includes which file or files are part of the datasets, their local paths, download locations, and verification information. We then try to make an optimal decision in getting the data to you as quickly as we can.\n",
    "\n",
    "1. If you are running on a resource with direct access to the ESGF holdings (such a Jupyter notebook on nimbus.llnl.gov), then we check if the dataset files are locally available. We have a handful of locations built-in to `intake-esgf` but you can also set a location manually with `cat.set_esgf_data_root()`.\n",
    "2. If a dataset has associated files that have been previously downloaded into the local cache, then we will load these files into memory.\n",
    "3. If no direct file access is found, then we will queue the dataset files for download. File downloads will occur in parallel from the locations which provide you the fastest transfer speeds. Initially we will randomize the download locations, but as you use `intake-esgf`, we keep track of which servers provide you fastest transfer speeds and future downloads will prefer these locations. Once downloaded, we check file validity, and load into `xarray` containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsd = cat.to_dataset_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that progress bars inform you that file information is being obtained\n",
    "and that downloads are taking place. As files are downloaded, they are placed into a\n",
    "local cache in `${HOME}/.esgf` in a directory structure that mirrors that of the\n",
    "remote storage. For future analysis which uses these datasets, `intake-esgf` will\n",
    "first check this cache to see if a file already exists and use it instead of\n",
    "re-downloading. Then it returns a dictionary whose keys are by default the minimal set\n",
    "of facets to uniquely describe a dataset in the current search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dsd.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the download process, you may have also noticed that a progress bar informed\n",
    "you that we were adding cell measures. If you have worked with ESGF data before, you\n",
    "know that cell measure information like `areacella` is needed to take proper\n",
    "area-weighted means/summations. Yet many times, model centers have not uploaded this\n",
    "information uniformly in all submissions. We perform a search for each dataset being\n",
    "placed in the dataset dictionary, progressively dropping dataset facets to find, if\n",
    "possible, the cell measures that are *closest* to the dataset being downloaded.\n",
    "Sometimes they are simply in another `variant_label`, but other times they could be in a\n",
    "different `activity_id`. No matter where they are, we find them for you and add them\n",
    "by default (disable with `to_dataset_dict(add_measures=False)`). \n",
    "\n",
    "We determine which measures need downloaded by looking in the dataset attributes. Since `tas` is an atmospheric variable, we will see that its `cell_measures = 'area: areacella'`. If you print this variable you will see that measure has been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsd[\"Amon.tas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for `gpp` we also need the land fractions, which is detected by the presence of `area: where land` in the `cell_methods`. You will notice that both `areacella` and `sftlf` are added to `Lmon.gpp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsd[\"Lmon.gpp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(6, 12), nrows=3)\n",
    "\n",
    "# temperature\n",
    "ds = dsd[\"Amon.tas\"][\"tas\"].mean(dim=\"time\") - 273.15  # to [C]\n",
    "ds.plot(ax=axs[0], cmap=\"bwr\", vmin=-40, vmax=40, cbar_kwargs={\"label\": \"tas [C]\"})\n",
    "\n",
    "# precipitation\n",
    "ds = dsd[\"Amon.pr\"][\"pr\"].mean(dim=\"time\") * 86400 / 999.8 * 1000  # to [mm d-1]\n",
    "ds.plot(ax=axs[1], cmap=\"Blues\", vmax=10, cbar_kwargs={\"label\": \"pr [mm d-1]\"})\n",
    "\n",
    "# gross primary productivty\n",
    "ds = dsd[\"Lmon.gpp\"][\"gpp\"].mean(dim=\"time\") * 86400 * 1000  # to [g m-2 d-1]\n",
    "ds.plot(ax=axs[2], cmap=\"Greens\", cbar_kwargs={\"label\": \"gpp [g m-2 d-1]\"})\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "`intake-esgf` becomes the way that you download or locate data as well as load it into memory. It is a full specification of what your analysis is about and makes your script portable to other machines or even in use with serverside computing. We are actively developing this codebase. Let us [know](https://github.com/esgf2-us/intake-esgf/issues) what other features you would like to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
